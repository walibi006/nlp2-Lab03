{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install datasets evaluate transformers[sentencepiece]\n! pip install -U git+https://github.com/huggingface/transformers.git\n! pip install -U git+https://github.com/huggingface/accelerate.git\nimport evaluate\nfrom datasets import load_dataset\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-05T18:46:46.041778Z","iopub.execute_input":"2023-06-05T18:46:46.042152Z","iopub.status.idle":"2023-06-05T18:48:25.295580Z","shell.execute_reply.started":"2023-06-05T18:46:46.042122Z","shell.execute_reply":"2023-06-05T18:48:25.294616Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /opt/conda/lib/python3.10/site-packages (4.29.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (10.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.14.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (2023.5.5)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.13.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.1.99)\nCollecting protobuf<=3.20.2 (from transformers[sentencepiece])\n  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nInstalling collected packages: protobuf, evaluate\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.4.1 requires cupy-cuda11x<12.0.0a0,>=9.5.0, which is not installed.\ncuml 23.4.1 requires cupy-cuda11x<12.0.0a0,>=9.5.0, which is not installed.\ndask-cudf 23.4.1 requires cupy-cuda11x<12.0.0a0,>=9.5.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 10.0.1 which is incompatible.\ncudf 23.4.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.2 which is incompatible.\ncuml 23.4.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\ndask-cudf 23.4.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\ngoogle-cloud-artifact-registry 1.8.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-datastore 2.15.2 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-dlp 3.12.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-pubsub 2.16.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-pubsub 2.16.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\ngoogle-cloud-resource-manager 1.10.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\ngoogle-cloud-spanner 3.33.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 1.33.2 which is incompatible.\nkfp 1.8.21 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.86.0 which is incompatible.\ntensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\ntensorflow-serving-api 2.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.0 protobuf-3.20.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-j7cy3fsu\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-j7cy3fsu\n  Resolved https://github.com/huggingface/transformers.git to commit 7631db0fdcfbd95b1f21d8034a0b8df73b9380ff\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (0.14.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0.dev0) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.0.dev0) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0.dev0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0.dev0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0.dev0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0.dev0) (2023.5.7)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.30.0.dev0-py3-none-any.whl size=7160170 sha256=976cc6520f871e39f4bc0d5e7721ac1d60121dd02c573b1c473c6cb393a35786\n  Stored in directory: /tmp/pip-ephem-wheel-cache-19767la7/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.29.2\n    Uninstalling transformers-4.29.2:\n      Successfully uninstalled transformers-4.29.2\nSuccessfully installed transformers-4.30.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/huggingface/accelerate.git\n  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-hh5veg37\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-hh5veg37\n  Resolved https://github.com/huggingface/accelerate.git to commit 16ca01feeaf59ff3d67fb6328c04628e31f1fe72\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (5.4.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.20.0.dev0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.0.dev0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.0.dev0) (1.3.0)\nBuilding wheels for collected packages: accelerate\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.20.0.dev0-py3-none-any.whl size=226997 sha256=8acf258c751ac631ea761e61acf1cf1f1b1bbf6ac92b49c9b7ee963c2927c752\n  Stored in directory: /tmp/pip-ephem-wheel-cache-caouywsn/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\nSuccessfully built accelerate\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.20.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nLoad the data from IMDB , creates a suitable tokenizer and tokenize the dataset.\n\ninput: \n    - String: the name of the checkpoint to use for the tokenizer\n    \noutput:\n    - Object: The tokenized dataset\n    - Object: The tokenizer\n    - String: The checkpoint name\n\"\"\"\ndef load_data(checkpoint):\n    raw_datasets = load_dataset(\"imdb\")\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n    tokenized_datasets = raw_datasets.map(lambda example: tokenizer.__call__(example[\"text\"], truncation=True), batched=True)\n    return tokenized_datasets, tokenizer, checkpoint\n\n\"\"\"\nCreates a model using the outputs of load_data.\n\ninput:\n    - Object: A tokenized dataset\n    - Object: A tokenizer\n    - String: The checkpoint name\n    - Int: Number of epoch to train\n    \noutput:\n    - Object: A trainer for the model\n    - Object: The tokenized dataset\n\"\"\"\ndef create_model(tokenized_datasets, tokenizer, checkpoint ,num_train_epochs=1):\n    training_args = TrainingArguments(\"test-trainer\", num_train_epochs=num_train_epochs, report_to=\"none\")\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n    trainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator= DataCollatorWithPadding(tokenizer=tokenizer),\n    tokenizer=tokenizer,\n    )\n    return trainer, tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2023-06-05T18:48:25.297634Z","iopub.execute_input":"2023-06-05T18:48:25.298067Z","iopub.status.idle":"2023-06-05T18:48:25.309407Z","shell.execute_reply.started":"2023-06-05T18:48:25.298033Z","shell.execute_reply":"2023-06-05T18:48:25.306885Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nFine tune the given model checkpoint with the IMDB dataset\n\ninput:\n    - String: the name of the checkpoint to fine tune\n\noutput:\n    - Object: The trainer for the model\n    - Object: The tokenized dataset used for fine tunig\n\"\"\"\ndef fine_tune(checkpoint):\n    trainer, tokenized_datasets = create_model(*load_data(checkpoint))\n    trainer.train()\n    predictions = trainer.predict(tokenized_datasets[\"test\"])\n    print(predictions.predictions.shape, predictions.label_ids.shape)\n    return trainer,tokenized_datasets\n\n\n\"\"\"\nCompute the accuracy of the given predictions\n\ninput:\n    - Object: A tokenized dataset\n    - Object: A trainer for the model to evaluate\n\noutput:\n    - Object: The trainer\n    - Object: the dataset\n\"\"\"\ndef compute_metrics(trainer, dataset):\n    predictions = trainer.predict(dataset[\"test\"])\n    predicted_labels = np.argmax(predictions.predictions, axis=1)\n    accuracy = np.mean(predicted_labels == dataset[\"test\"][\"label\"]) * 100\n    print(\"Accuracy:\", accuracy)\n    return trainer, dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T18:48:25.311063Z","iopub.execute_input":"2023-06-05T18:48:25.311630Z","iopub.status.idle":"2023-06-05T18:48:25.321832Z","shell.execute_reply.started":"2023-06-05T18:48:25.311599Z","shell.execute_reply":"2023-06-05T18:48:25.320958Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(compute_metrics(*fine_tune(\"distilbert-base-uncased\"))) # very long to run","metadata":{"execution":{"iopub.status.busy":"2023-06-05T18:48:25.324210Z","iopub.execute_input":"2023-06-05T18:48:25.324606Z","iopub.status.idle":"2023-06-05T19:09:29.618093Z","shell.execute_reply.started":"2023-06-05T18:48:25.324571Z","shell.execute_reply":"2023-06-05T19:09:29.617048Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca52c68cf4640ada9403d7378b7885a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001da39e43344d92ac7d982331318be1"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc728b74ca64322b5bd4b395a7fa569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9605b9d3aceb4a808a8db934e30cd7ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"775b78378c2d414eafd8a4b617886e8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdcfa2cbf67e4f3095d246f6abc70680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1215b05753374bb0a8c4f03920b76c42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faa7eb8bc55d4fcc8c380513c61e4523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ae43b826080458399d3f05ed15d07c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48eb56970824b1188c6b622cadabc74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb001f64eb14f1a86107c9819859387"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6765890254864077bcb14d8bff2dad50"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3125/3125 11:19, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.403800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.334900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.311900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.279400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.265300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.269000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"(25000, 2) (25000,)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Accuracy: 92.624\n(<transformers.trainer.Trainer object at 0x7c1a402b7a00>, DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50000\n    })\n}))\n","output_type":"stream"}]},{"cell_type":"code","source":"mvw_chechpoint = \"mvonwyl/distilbert-base-uncased-imdb\"\n\ntrainer, dataset = compute_metrics(*create_model(*load_data(mvw_chechpoint)))","metadata":{"execution":{"iopub.status.busy":"2023-06-05T19:11:51.873182Z","iopub.execute_input":"2023-06-05T19:11:51.873607Z","iopub.status.idle":"2023-06-05T19:17:17.576752Z","shell.execute_reply.started":"2023-06-05T19:11:51.873564Z","shell.execute_reply":"2023-06-05T19:17:17.575624Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad01d145dd442b6914fe18fe6b63b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/360 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a20a5efec0b442187854b9baf86eb98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf292efbe0424e6497feef429cf7a17e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88511cc4d9974a429cb8dfbe10c2f8f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24bf9781adc8498c9777f853aa88f49e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0a4ca11dc2d4aed8d83863ddb1d55ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8675e7cb1f6c4cd3958e2a825e4946fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c53ce53b6c7432f9987cb13e1132ba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ceab73360e4e73b90ebbab82ed5493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e13a8d8cc7ed4935a34e35a95ac2935d"}},"metadata":{}},{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Accuracy: 92.948\n","output_type":"stream"}]},{"cell_type":"code","source":"X = dataset[\"test\"][\"text\"]\nactual_labels = dataset[\"test\"][\"label\"]\npredicted_labels = np.argmax(trainer.predict(dataset[\"test\"]).predictions, axis=1)\nwrong = [[X[i],predicted_labels[i], actual_labels[i]] for i in range(len(X)) if predicted_labels[i] != actual_labels[i]]\nwrong[:3]","metadata":{"execution":{"iopub.status.busy":"2023-06-05T19:26:22.208945Z","iopub.execute_input":"2023-06-05T19:26:22.209339Z","iopub.status.idle":"2023-06-05T19:30:00.452486Z","shell.execute_reply.started":"2023-06-05T19:26:22.209309Z","shell.execute_reply":"2023-06-05T19:30:00.451464Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[[\"First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\",\n  1,\n  0],\n [\"I'm the type of guy who loves hood movies from New Jack City to Baby Boy to Killa Season, from the b grade to the Hollywood. but this movie was something different. i am no hater and this movie was kinda enjoyable. but some bits were just weird. well the acting wasn't to good, compared to Silkk The Shockers performance in Hot Boyz (quite good) and Ice-T in new Jack and SVU (great). the scene where Corrupt (Ice-T) kills the wanna be Jamaican dude he says something and lights himself on fire burning both Ice-T and the other dude, this kills the Jamaican, however Ice-T is unharmed, very similar to Ice's other movie Urban Menace (which stars both of these actors) were Snoops character is supernatural, however after this there is nothing suggested that Corrupt is like a demon. When MJ (Silkk) gets stabbed at first he struggling but after that he fights normally and was stabbed in the thigh-WITH OUT BLOOD. and when MJ confesses killing a cop cos the cop was beating up his friend Benny was weird, Benny isn't introduced in this movie and the scene isn't in the film. it does hold weight to the fact why Corrupt wants to kill MJ but is still makes u scratch your head. wen Jody writes a letter to Miss Jones character explaining what happened to them afterwords doesn't mention what happen 2 other main characters MJ and Lisa. the film did show the horror and poverty of the ghetto-which plagues the lives of Latinos and Blacks word wide-was a good part of the film, even though the clip of the projects was re-used thousands of times. and the scene where Miles kills the Latino brother by crashing his bike at full speed (not wearing a helmet) and running into my Latino brothers car would of killed him. the movie was similar to the film Urban Menace and half the actors were in both of these movies as well as the production team. it was OK tho. but me being from poverty i love hood films, however if u don't love em like i do Don't WATCH IT. only thing saving me from walking out is it reminded me of the first movie i made which was made with 100 dollars, and my love of the genre.<br /><br />Nathaniel Purez\",\n  1,\n  0],\n [\"The only reason this movie is not given a 1 (awful) vote is that the acting of both Ida Lupino and Robert Ryan is superb. Ida Lupino who is lovely, as usual, becomes increasingly distraught as she tries various means to rid herself of a madman. Robert Ryan is terrifying as the menacing stranger whose character, guided only by his disturbed mind, changes from one minute to the next. Seemingly simple and docile, suddenly he becomes clever and threatening. Ms. Lupino's character was in more danger from that house she lived in and her own stupidity than by anyone who came along. She could not manage to get out of her of her own house: windows didn't open, both front and back doors locked and unlocked from the inside with a key. You could not have designed a worse fire-trap if you tried. She did not take the precaution of having even one extra key. Nor could she figure out how to summon help from nearby neighbors or get out of her own basement while she was locked in and out of sight of her captor. I don't know what war her husband was killed in, but if it was World War II, the furnishings in her house, the styles of the clothes, especially the children and the telephone company repairman's car are clearly anachronistic. I recommend watching this movie just to see what oddities you can find.\",\n  1,\n  0]]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Wrongly classified comments\n\n\n* Predicted positive, actual negative :\\\nFirst off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\")\n\n\nTo be honest, as a human I thought it was a positive comment when reading it, but it's labeled negative in IMDB. The reason why the model didn't classified this comment as negative is because the autor puts emphasis on the fact that it would be enjoyable to Van Damme fans. The model can be tricked by expressions like :\n\"Good fun stuff!\"\n\"This movie is much better than any of the movies [...]\"\n\"in my opinion it's worth watching.\"\n\n\n* Predicted positive, actual negative :\\\nThe only reason this movie is not given a 1 (awful) vote is that the acting of both Ida Lupino and Robert Ryan is superb. Ida Lupino who is lovely, as usual, becomes increasingly distraught as she tries various means to rid herself of a madman. Robert Ryan is terrifying as the menacing stranger whose character, guided only by his disturbed mind, changes from one minute to the next. Seemingly simple and docile, suddenly he becomes clever and threatening. Ms. Lupino's character was in more danger from that house she lived in and her own stupidity than by anyone who came along. She could not manage to get out of her of her own house: windows didn't open, both front and back doors locked and unlocked from the inside with a key. You could not have designed a worse fire-trap if you tried. She did not take the precaution of having even one extra key. Nor could she figure out how to summon help from nearby neighbors or get out of her own basement while she was locked in and out of sight of her captor. I don't know what war her husband was killed in, but if it was World War II, the furnishings in her house, the styles of the clothes, especially the children and the telephone company repairman's car are clearly anachronistic. I recommend watching this movie just to see what oddities you can find.\n\n\nThe author once again litteraly says in this comment that they recommend watching the movie. They clearly are laughing at what seems like a B-movie with 2 good actors that saves it a bit but the clear compliments like \"both Ida Lupino and Robert Ryan is superb\", \"Ida Lupino who is lovely, as usual\" and \"I recommend watching this movie\" may lead the model to classifyit as positive. \n\n* Predicted negative, actual positive :\\\nAlthough Bullet In The Brain is, without question, superior amongst short films, it largely seems more like a short piece of writing than a film. And it is a little hard to feel too sorry for the teacher when his smart ass remarks get him shot. But after the bullet enters his brain we begin to understand a little bit about why he became so jaded with life in the first place. There is an awful amount of detail packed into this reasonably short film and this is what makes me feel that it should have been extended a little bit - it seems like there's almost too much to take in at once as the details come flying at you so fast. A slightly more relaxed pace and a less po-faced narrator in the final section would have benefitted this film a little bit. Despite these complaints, there is no denying that Bullet In The Brain is a quite stupendous work compared to many short, and even full length films. The makers should be applauded for trying to make such a basically emotional and literate film in the current climate of quick jokes and Hollywood action.\n\n\nIn this comment the author tries to hint on some ways to make the movie better. They tought that it would have a good potential, but also that it's a bit wasted. The fact that they use constructive criticism throughout the whole comment may be why it was labelled negative.","metadata":{}},{"cell_type":"markdown","source":"## Advantages of Transformers:\n\n1. Capturing Long-Term Dependencies: Transformers are good at capturing long-term dependencies in sequential data due to their self-attention mechanism. They can learn dependencies between distant positions in the input, which is good for tasks such as machine translation or document classification.\n\n2. Parallelizable Computation: Transformers can process inputs in parallel, unlike recurrent models that process inputs sequentially. This parallelization makes transformers more efficient during training and inference and faster processing times.\n\n3. Scalability to Large Datasets: Transformers can handle large datasets effectively. They can be trained on massive amounts of data, taking advantage of distributed computing and GPUs, which helps in achieving better performance on complex tasks.\n\n4. No Sequential Constraints: Transformers do not have sequential constraints, meaning they can process the entire input sequence at once. This makes them more flexible in handling different input lengths and allows for easier implementation and optimization.\n\n## Disadvantages of Transformers:\n1. High Computational Requirements: Transformers require significant computational resources due to their large number of parameters and self-attention mechanism. Training and fine-tuning transformer models can be computationally expensive and time-consuming, especially for large-scale models.\n\n2. Complex Architecture and Training: Transformers have a more complex architecture compared to simpler models like naive Bayes. They require careful hyperparameter tuning, large amounts of training data, and longer training times to achieve optimal performance.\n\n3. Lack of Interpretable Features: Transformers learn complex representations of the input data, making it challenging to interpret the model's internal workings and understand the specific features it relies on for predictions. Naive Bayes, on the other hand, provides interpretable features based on conditional probabilities.\n\n4. Data Dependency: Transformers heavily rely on large amounts of labeled data for training. They might not perform well in scenarios where limited labeled data is available, especially in niche domains lacking annotated datasets.\n\n## Comparing with Naive Bayes:\n- Advantage: Transformers can capture complex relationships and dependencies in data, whereas Naive Bayes assumes independence between features. Transformers are more suitable for tasks that require understanding the context and capturing long-range dependencies.\n- Advantage: Transformers do not have the restrictive assumptions of Naive Bayes, allowing them to handle a wider range of data distributions and complex patterns.\n- Disadvantage: Transformers require significantly more computational resources, training data, and training time compared to Naive Bayes, making them less suitable for simple tasks or scenarios with limited resources.\n\n## Comparing with Recurrent Models (RNNs/LSTMs):\n- Advantage: Transformers can capture long-term dependencies more effectively than recurrent models. RNNs/LSTMs suffer from vanishing or exploding gradients and struggle with long-range dependencies, while transformers use self-attention to attend to all positions in the input sequence.\n- Advantage: Transformers have parallelizable computation, making them more efficient for processing long sequences and larger datasets. RNNs/LSTMs process inputs sequentially, which can be a bottleneck for training and inference speed.\n- Disadvantage: Transformers generally require more computational resources and training time compared to recurrent models. Transformers have a higher model complexity and may require larger amounts of data for training.\n- Disadvantage: Recurrent models have a sequential nature, making them better suited for tasks that involve sequential processing or modeling temporal dependencies, such as time series forecasting or text generation.\n\nThe choice between using a transformer, naive Bayes, or a recurrent model depends on the specific task, available resources, dataset size, and the nature of the data. It's important to consider these factors when selecting the appropriate model for production use.","metadata":{}}]}
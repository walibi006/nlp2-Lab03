{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-05T18:46:46.042152Z","iopub.status.busy":"2023-06-05T18:46:46.041778Z","iopub.status.idle":"2023-06-05T18:48:25.295580Z","shell.execute_reply":"2023-06-05T18:48:25.294616Z","shell.execute_reply.started":"2023-06-05T18:46:46.042122Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\jo81\\Documents\\Epita\\scia\\nlp2-Lab03\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset\n","import numpy as np\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## NLP 2  LAB 3: HuggingFace Transformers\n","Auhors : \n","- Jonathan Poelger\n","- Ethan Machavoine\n","- Aurelien Rouxel"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-05T18:48:25.298067Z","iopub.status.busy":"2023-06-05T18:48:25.297634Z","iopub.status.idle":"2023-06-05T18:48:25.309407Z","shell.execute_reply":"2023-06-05T18:48:25.306885Z","shell.execute_reply.started":"2023-06-05T18:48:25.298033Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Load the data from IMDB , creates a suitable tokenizer and tokenize the dataset.\n","\n","input: \n","    - String: the name of the checkpoint to use for the tokenizer\n","    \n","output:\n","    - Object: The tokenized dataset\n","    - Object: The tokenizer\n","    - String: The checkpoint name\n","\"\"\"\n","def load_data(checkpoint):\n","    raw_datasets = load_dataset(\"imdb\")\n","    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","    tokenized_datasets = raw_datasets.map(lambda example: tokenizer.__call__(example[\"text\"], truncation=True), batched=True)\n","    return tokenized_datasets, tokenizer, checkpoint\n","\n","\"\"\"\n","Creates a model using the outputs of load_data.\n","\n","input:\n","    - Object: A tokenized dataset\n","    - Object: A tokenizer\n","    - String: The checkpoint name\n","    - Int: Number of epoch to train\n","    \n","output:\n","    - Object: A trainer for the model\n","    - Object: The tokenized dataset\n","\"\"\"\n","def create_model(tokenized_datasets, tokenizer, checkpoint ,num_train_epochs=1):\n","    training_args = TrainingArguments(\"test-trainer\", num_train_epochs=num_train_epochs, report_to=\"none\")\n","    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n","    trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"test\"],\n","    data_collator= DataCollatorWithPadding(tokenizer=tokenizer),\n","    tokenizer=tokenizer,\n","    )\n","    return trainer, tokenized_datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-05T18:48:25.311630Z","iopub.status.busy":"2023-06-05T18:48:25.311063Z","iopub.status.idle":"2023-06-05T18:48:25.321832Z","shell.execute_reply":"2023-06-05T18:48:25.320958Z","shell.execute_reply.started":"2023-06-05T18:48:25.311599Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Fine tune the given model checkpoint with the IMDB dataset\n","\n","input:\n","    - String: the name of the checkpoint to fine tune\n","\n","output:\n","    - Object: The trainer for the model\n","    - Object: The tokenized dataset used for fine tunig\n","\"\"\"\n","def fine_tune(checkpoint):\n","    trainer, tokenized_datasets = create_model(*load_data(checkpoint))\n","    trainer.train()\n","    predictions = trainer.predict(tokenized_datasets[\"test\"])\n","    print(predictions.predictions.shape, predictions.label_ids.shape)\n","    return trainer,tokenized_datasets\n","\n","\n","\"\"\"\n","Compute the accuracy of the given predictions\n","\n","input:\n","    - Object: A tokenized dataset\n","    - Object: A trainer for the model to evaluate\n","\n","output:\n","    - Object: The trainer\n","    - Object: the dataset\n","\"\"\"\n","def compute_metrics(trainer, dataset):\n","    predictions = trainer.predict(dataset[\"test\"])\n","    predicted_labels = np.argmax(predictions.predictions, axis=1)\n","    accuracy = np.mean(predicted_labels == dataset[\"test\"][\"label\"]) * 100\n","    print(\"Accuracy:\", accuracy)\n","    return trainer, dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-05T18:48:25.324606Z","iopub.status.busy":"2023-06-05T18:48:25.324210Z","iopub.status.idle":"2023-06-05T19:09:29.618093Z","shell.execute_reply":"2023-06-05T19:09:29.617048Z","shell.execute_reply.started":"2023-06-05T18:48:25.324571Z"},"trusted":true},"outputs":[],"source":["# Create a model with the bert-base-uncased checkpoint\n","# Fine tune it with the IMDB dataset on one epoch\n","# Compute the accuracy of the model\n","print(compute_metrics(*fine_tune(\"distilbert-base-uncased\"))) # very long to run"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-05T19:11:51.873607Z","iopub.status.busy":"2023-06-05T19:11:51.873182Z","iopub.status.idle":"2023-06-05T19:17:17.576752Z","shell.execute_reply":"2023-06-05T19:17:17.575624Z","shell.execute_reply.started":"2023-06-05T19:11:51.873564Z"},"trusted":true},"outputs":[],"source":["# Checkpoint for fine tuned model\n","mvw_chechpoint = \"mvonwyl/distilbert-base-uncased-imdb\"\n","\n","# Load the fine tuned model and compute its accuracy\n","trainer, dataset = compute_metrics(*create_model(*load_data(mvw_chechpoint)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-05T19:26:22.209339Z","iopub.status.busy":"2023-06-05T19:26:22.208945Z","iopub.status.idle":"2023-06-05T19:30:00.452486Z","shell.execute_reply":"2023-06-05T19:30:00.451464Z","shell.execute_reply.started":"2023-06-05T19:26:22.209309Z"},"trusted":true},"outputs":[],"source":["# Print the first 3 wrong predictions\n","X = dataset[\"test\"][\"text\"]\n","actual_labels = dataset[\"test\"][\"label\"]\n","predicted_labels = np.argmax(trainer.predict(dataset[\"test\"]).predictions, axis=1)\n","wrong = [[X[i],predicted_labels[i], actual_labels[i]] for i in range(len(X)) if predicted_labels[i] != actual_labels[i]]\n","wrong[:3]"]},{"cell_type":"markdown","metadata":{},"source":["## Wrongly classified comments\n","\n","\n","* Predicted positive, actual negative :\\\n","First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\")\n","\n","\n","To be honest, as a human I thought it was a positive comment when reading it, but it's labeled negative in IMDB. The reason why the model didn't classified this comment as negative is because the autor puts emphasis on the fact that it would be enjoyable to Van Damme fans. The model can be tricked by expressions like :\n","\"Good fun stuff!\"\n","\"This movie is much better than any of the movies [...]\"\n","\"in my opinion it's worth watching.\"\n","\n","\n","* Predicted positive, actual negative :\\\n","The only reason this movie is not given a 1 (awful) vote is that the acting of both Ida Lupino and Robert Ryan is superb. Ida Lupino who is lovely, as usual, becomes increasingly distraught as she tries various means to rid herself of a madman. Robert Ryan is terrifying as the menacing stranger whose character, guided only by his disturbed mind, changes from one minute to the next. Seemingly simple and docile, suddenly he becomes clever and threatening. Ms. Lupino's character was in more danger from that house she lived in and her own stupidity than by anyone who came along. She could not manage to get out of her of her own house: windows didn't open, both front and back doors locked and unlocked from the inside with a key. You could not have designed a worse fire-trap if you tried. She did not take the precaution of having even one extra key. Nor could she figure out how to summon help from nearby neighbors or get out of her own basement while she was locked in and out of sight of her captor. I don't know what war her husband was killed in, but if it was World War II, the furnishings in her house, the styles of the clothes, especially the children and the telephone company repairman's car are clearly anachronistic. I recommend watching this movie just to see what oddities you can find.\n","\n","\n","The author once again litteraly says in this comment that they recommend watching the movie. They clearly are laughing at what seems like a B-movie with 2 good actors that saves it a bit but the clear compliments like \"both Ida Lupino and Robert Ryan is superb\", \"Ida Lupino who is lovely, as usual\" and \"I recommend watching this movie\" may lead the model to classifyit as positive. \n","\n","* Predicted negative, actual positive :\\\n","Although Bullet In The Brain is, without question, superior amongst short films, it largely seems more like a short piece of writing than a film. And it is a little hard to feel too sorry for the teacher when his smart ass remarks get him shot. But after the bullet enters his brain we begin to understand a little bit about why he became so jaded with life in the first place. There is an awful amount of detail packed into this reasonably short film and this is what makes me feel that it should have been extended a little bit - it seems like there's almost too much to take in at once as the details come flying at you so fast. A slightly more relaxed pace and a less po-faced narrator in the final section would have benefitted this film a little bit. Despite these complaints, there is no denying that Bullet In The Brain is a quite stupendous work compared to many short, and even full length films. The makers should be applauded for trying to make such a basically emotional and literate film in the current climate of quick jokes and Hollywood action.\n","\n","\n","In this comment the author tries to hint on some ways to make the movie better. They tought that it would have a good potential, but also that it's a bit wasted. The fact that they use constructive criticism throughout the whole comment may be why it was labelled negative."]},{"cell_type":"markdown","metadata":{},"source":["## Advantages of Transformers:\n","\n","1. Capturing Long-Term Dependencies: Transformers are good at capturing long-term dependencies in sequential data due to their self-attention mechanism. They can learn dependencies between distant positions in the input, which is good for tasks such as machine translation or document classification.\n","\n","2. Parallelizable Computation: Transformers can process inputs in parallel, unlike recurrent models that process inputs sequentially. This parallelization makes transformers more efficient during training and inference and faster processing times.\n","\n","3. Scalability to Large Datasets: Transformers can handle large datasets effectively. They can be trained on massive amounts of data, taking advantage of distributed computing and GPUs, which helps in achieving better performance on complex tasks.\n","\n","4. No Sequential Constraints: Transformers do not have sequential constraints, meaning they can process the entire input sequence at once. This makes them more flexible in handling different input lengths and allows for easier implementation and optimization.\n","\n","## Disadvantages of Transformers:\n","1. High Computational Requirements: Transformers require significant computational resources due to their large number of parameters and self-attention mechanism. Training and fine-tuning transformer models can be computationally expensive and time-consuming, especially for large-scale models.\n","\n","2. Complex Architecture and Training: Transformers have a more complex architecture compared to simpler models like naive Bayes. They require careful hyperparameter tuning, large amounts of training data, and longer training times to achieve optimal performance.\n","\n","3. Lack of Interpretable Features: Transformers learn complex representations of the input data, making it challenging to interpret the model's internal workings and understand the specific features it relies on for predictions. Naive Bayes, on the other hand, provides interpretable features based on conditional probabilities.\n","\n","4. Data Dependency: Transformers heavily rely on large amounts of labeled data for training. They might not perform well in scenarios where limited labeled data is available, especially in niche domains lacking annotated datasets.\n","\n","## Comparing with Naive Bayes:\n","- Advantage: Transformers can capture complex relationships and dependencies in data, whereas Naive Bayes assumes independence between features. Transformers are more suitable for tasks that require understanding the context and capturing long-range dependencies.\n","- Advantage: Transformers do not have the restrictive assumptions of Naive Bayes, allowing them to handle a wider range of data distributions and complex patterns.\n","- Disadvantage: Transformers require significantly more computational resources, training data, and training time compared to Naive Bayes, making them less suitable for simple tasks or scenarios with limited resources.\n","\n","## Comparing with Recurrent Models (RNNs/LSTMs):\n","- Advantage: Transformers can capture long-term dependencies more effectively than recurrent models. RNNs/LSTMs suffer from vanishing or exploding gradients and struggle with long-range dependencies, while transformers use self-attention to attend to all positions in the input sequence.\n","- Advantage: Transformers have parallelizable computation, making them more efficient for processing long sequences and larger datasets. RNNs/LSTMs process inputs sequentially, which can be a bottleneck for training and inference speed.\n","- Disadvantage: Transformers generally require more computational resources and training time compared to recurrent models. Transformers have a higher model complexity and may require larger amounts of data for training.\n","- Disadvantage: Recurrent models have a sequential nature, making them better suited for tasks that involve sequential processing or modeling temporal dependencies, such as time series forecasting or text generation.\n","\n","The choice between using a transformer, naive Bayes, or a recurrent model depends on the specific task, available resources, dataset size, and the nature of the data. It's important to consider these factors when selecting the appropriate model for production use."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}

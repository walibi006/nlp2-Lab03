{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, BertConfig, BertModel\n",
    "from transformers import AutoTokenizer, AdamW, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import get_scheduler, CamembertTokenizer, CamembertForMaskedLM, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    "\n",
    "def tutorial_1(): \n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "    print(classifier(\"I've been waiting for a HuggingFace course my whole life.\"))\n",
    "    print(classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]))\n",
    "    \n",
    "    classifier = pipeline(\"zero-shot-classification\")\n",
    "    print(classifier(\"This is a course about the Transformers library\", candidate_labels=[\"education\", \"politics\", \"business\"],))\n",
    "    \n",
    "    generator = pipeline(\"text-generation\")\n",
    "    print(generator(\"In this course, we will teach you how to\"))\n",
    "    \n",
    "    unmasker = pipeline(\"fill-mask\")\n",
    "    print(unmasker(\"This course will teach you all about <mask> models.\", top_k=2))\n",
    "    \n",
    "    ner = pipeline(\"ner\", grouped_entities=True)\n",
    "    print(ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))\n",
    "    \n",
    "    question_answerer = pipeline(\"question-answering\")\n",
    "    print(question_answerer(question=\"Where do I work?\", context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",))\n",
    "    \n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    print(summarizer(text))\n",
    "    \n",
    "    translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "    print(translator(\"Ce cours est produit par Hugging Face.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_2():\n",
    "    unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "    result = unmasker(\"This man works as a [MASK].\")\n",
    "    print([r[\"token_str\"] for r in result])\n",
    "\n",
    "    result = unmasker(\"This woman works as a [MASK].\")\n",
    "    print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_3():\n",
    "    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    ]\n",
    "    inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "    \n",
    "    model = AutoModel.from_pretrained(checkpoint)\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs.last_hidden_state.shape)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs.logits.shape)\n",
    "    print(outputs.logits)\n",
    "    \n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    print(predictions)\n",
    "    \n",
    "    print (model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_4():\n",
    "    config = BertConfig()\n",
    "    model = BertModel(config)\n",
    "    print(config)\n",
    "    \n",
    "    model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "    model.save_pretrained(\"pretrained_models/bert-custom\")\n",
    "    \n",
    "    sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "    encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102], ]\n",
    "    \n",
    "    model_inputs = torch.tensor(encoded_sequences)\n",
    "    output = model(model_inputs)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_5():\n",
    "    tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "    print(tokenized_text)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    print(tokenizer(\"Using a Transformer network is simple\"))\n",
    "    tokenizer.save_pretrained(\"directory_on_my_computer\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    sequence = \"Using a Transformer network is simple\"\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    print(tokens)\n",
    "    \n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(ids)\n",
    "    \n",
    "    decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "    print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_6():\n",
    "    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "    sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor([ids])\n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    output = model(input_ids)\n",
    "    print(\"Logits:\", output.logits)\n",
    "    \n",
    "    sequence1_ids = [[200, 200, 200]]\n",
    "    sequence2_ids = [[200, 200]]\n",
    "    batched_ids = [\n",
    "        [200, 200, 200],\n",
    "        [200, 200, tokenizer.pad_token_id],\n",
    "    ]\n",
    "\n",
    "    print(model(torch.tensor(sequence1_ids)).logits)\n",
    "    print(model(torch.tensor(sequence2_ids)).logits)\n",
    "    print(model(torch.tensor(batched_ids)).logits)\n",
    "    \n",
    "    batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "    ]\n",
    "    attention_mask = [\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 0],\n",
    "    ]\n",
    "    outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "    print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_7():\n",
    "    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "    model_inputs = tokenizer(sequence)\n",
    "    print(model_inputs[\"input_ids\"])\n",
    "\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(ids)\n",
    "    print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "    print(tokenizer.decode(ids))\n",
    "    \n",
    "    sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "    tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(**tokens)\n",
    "    print(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_8():\n",
    "    checkpoint = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    sequences = [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"This course is amazing!\",\n",
    "    ]\n",
    "    batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # This is new\n",
    "    batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    loss = model(**batch).loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "    print(raw_datasets)\n",
    "    raw_train_dataset = raw_datasets[\"train\"]\n",
    "    print(raw_train_dataset[0])\n",
    "    print(raw_train_dataset.features)\n",
    "    \n",
    "    checkpoint = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "    tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
    "    inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "    print(inputs)\n",
    "    \n",
    "    tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "    \n",
    "    tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    )\n",
    "    \n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "    \n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    print(tokenized_datasets)\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    samples = tokenized_datasets[\"train\"][:8]\n",
    "    samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "    print([len(x) for x in samples[\"input_ids\"]])\n",
    "    \n",
    "    batch = data_collator(samples)\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_9():\n",
    "    raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "    checkpoint = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\"test-trainer\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "    print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "    \n",
    "    def compute_metrics(eval_preds):\n",
    "        metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    print(compute_metrics(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_10():\n",
    "    raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "    checkpoint = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    \n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    print(tokenized_datasets[\"train\"].column_names)\n",
    "    \n",
    "    train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "    eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        break\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    outputs = model(**batch)\n",
    "    print(outputs.loss, outputs.logits.shape)\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    num_epochs = 1\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    print(num_training_steps)\n",
    "    \n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial_11():\n",
    "    camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
    "    results = camembert_fill_mask(\"Le camembert est <mask> :)\")\n",
    "    print(results)\n",
    "    \n",
    "    tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "    model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tutorial():\n",
    "    tutorial_1()\n",
    "    tutorial_2()\n",
    "    tutorial_3()\n",
    "    tutorial_4()\n",
    "    tutorial_5()\n",
    "    tutorial_6()\n",
    "    tutorial_7()\n",
    "    tutorial_8()\n",
    "    #tutorial_9() # tutorials with trainings takes a lot of time\n",
    "    #tutorial_10() \n",
    "    tutorial_11()\n",
    "\n",
    "#tutorial() # Run all tutorials, takes a lot of time"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
